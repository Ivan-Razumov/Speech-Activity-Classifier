{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is for run in kaggle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/silero-audio-classifier/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.io.wavfile import read\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take part of all data because full dataset is too large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PART = 0.05 # part of full data used to train our model \n",
    "VAL_PART = 0.2 # part of train data for validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/silero-audio-classifier/train.csv')\n",
    "_, train_df, _, _ = train_test_split(train_df, train_df['label'].values, test_size = TRAIN_PART, stratify = train_df['label'].values)\n",
    "train, val, _, _ = train_test_split(train_df, train_df['label'].values, test_size = VAL_PART, stratify = train_df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(label, num_class = 3):\n",
    "    one_hot = torch.zeros(num_class)\n",
    "    one_hot[label] = 1\n",
    "    return one_hot\n",
    "\n",
    "def read_audio(path):\n",
    "            sr, wav = read(path)\n",
    "            assert sr == 16000\n",
    "            assert len(wav) == 16000 * 3\n",
    "            assert len(wav.shape) == 1\n",
    "            return wav\n",
    "        \n",
    "def read_audio_norm(path):\n",
    "            wav = read_audio(path)\n",
    "            abs_max = np.abs(wav).max()\n",
    "            wav = wav.astype('float32')\n",
    "            if abs_max > 0:\n",
    "                wav *= 1 / abs_max\n",
    "            return wav\n",
    "\n",
    "window_size = 0.02\n",
    "window_stride = 0.01\n",
    "sample_rate = 16000\n",
    "\n",
    "n_fft = int(sample_rate * (window_size + 1e-8))\n",
    "win_length = n_fft\n",
    "hop_length = int(sample_rate * (window_stride + 1e-8))\n",
    "\n",
    "kwargs = {\n",
    "    'n_fft': n_fft,\n",
    "    'hop_length': hop_length,\n",
    "    'win_length': n_fft\n",
    "}\n",
    "\n",
    "def stft(wav):\n",
    "    D = librosa.stft(wav,\n",
    "                     **kwargs)\n",
    "    mag, phase = librosa.magphase(D)    \n",
    "    return mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, test = False, data_path = '/kaggle/input/silero-audio-classifier/train'):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "                \n",
    "        label_dict = {'speech':0,\n",
    "                      'music':1,\n",
    "                      'noise':2}\n",
    "        \n",
    "        wav = read_audio_norm(os.path.join(self.data_path, self.df.iloc[idx].wav_path))\n",
    "        mag = torch.tensor(stft(wav), dtype = torch.float32).permute(1,0) #.unsqueeze(0)\n",
    "        \n",
    "        if not self.test:\n",
    "            label = self.df.iloc[idx].target #label_dict[self.df.iloc[idx].label]\n",
    "            label_one_hot = to_onehot(label)\n",
    "            return mag, label_one_hot, torch.tensor(label, dtype = torch.int)\n",
    "        \n",
    "        return mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RNN-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, h_size = 256, num_class = 3, n_layers = 3, bidirectional = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size = 161, hidden_size = h_size, batch_first = True, num_layers = n_layers, bidirectional = bidirectional)\n",
    "        n_directions = 2 if bidirectional else 1\n",
    "        self.fc = nn.Linear(h_size*n_layers*n_directions, num_class)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, h = self.rnn(x)\n",
    "        n_layers, batch_size, h_size = h.shape\n",
    "        h = h.permute(1,0,2).reshape(batch_size, h_size*n_layers)\n",
    "        return self.softmax(self.fc(h))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SoundDataset(train)\n",
    "val_dataset = SoundDataset(val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 15\n",
    "lr = 3e-4\n",
    "\n",
    "model = RNNModel()\n",
    "optimizer = Adam(model.parameters(), lr = lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer=optimizer, mode='max', patience=1, verbose=True, factor=0.2)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0...\n",
      "Validation...\n",
      "vall accuracy is 0.8615384615384616\n",
      "Model saved at 0 epoch\n",
      "Training epoch 1...\n",
      "Validation...\n",
      "vall accuracy is 0.9168498168498168\n",
      "Model saved at 1 epoch\n",
      "Training epoch 2...\n",
      "Validation...\n",
      "vall accuracy is 0.9384615384615385\n",
      "Model saved at 2 epoch\n",
      "Training epoch 3...\n",
      "Validation...\n",
      "vall accuracy is 0.9351648351648352\n",
      "Training epoch 4...\n",
      "Validation...\n",
      "vall accuracy is 0.945054945054945\n",
      "Model saved at 4 epoch\n",
      "Training epoch 5...\n",
      "Validation...\n",
      "vall accuracy is 0.9326007326007326\n",
      "Training epoch 6...\n",
      "Validation...\n",
      "vall accuracy is 0.9545787545787546\n",
      "Model saved at 6 epoch\n",
      "Training epoch 7...\n",
      "Validation...\n",
      "vall accuracy is 0.9454212454212454\n",
      "Training epoch 8...\n",
      "Validation...\n",
      "vall accuracy is 0.954945054945055\n",
      "Model saved at 8 epoch\n",
      "Training epoch 9...\n",
      "Validation...\n",
      "vall accuracy is 0.9498168498168498\n",
      "Training epoch 10...\n",
      "Validation...\n",
      "vall accuracy is 0.9556776556776557\n",
      "Model saved at 10 epoch\n",
      "Training epoch 11...\n",
      "Validation...\n",
      "vall accuracy is 0.9468864468864469\n",
      "Training epoch 12...\n",
      "Validation...\n",
      "vall accuracy is 0.9457875457875458\n",
      "Epoch    13: reducing learning rate of group 0 to 6.0000e-05.\n",
      "Training epoch 13...\n",
      "Validation...\n",
      "vall accuracy is 0.9593406593406594\n",
      "Model saved at 13 epoch\n",
      "Training epoch 14...\n",
      "Validation...\n",
      "vall accuracy is 0.9600732600732601\n",
      "Model saved at 14 epoch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#tkepoch = tqdm(range(n_epoch), total = n_epoch)\n",
    "model.to(device)\n",
    "best_val_acc = 0\n",
    "patience = 3\n",
    "\n",
    "for i in range(n_epoch):\n",
    "     \n",
    "    \n",
    "    print(f\"Training epoch {i}...\")\n",
    "    epoch_train_loss = 0\n",
    "    model.train()\n",
    "    #tkloader = tqdm(train_loader, total = len(train_loader))\n",
    "    for x, y, _ in train_loader:\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_train_loss += loss.item()\n",
    "    \n",
    "    #print(f\"Epoch {i} loss is {epoch_train_loss}\")\n",
    "    print(\"Validation...\")\n",
    "    model.eval()\n",
    "    #tkval = tqdm(val_loader, total = len(val_loader))\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for x, y, l in val_loader:\n",
    "        with torch.no_grad():\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            #print(y_pred.shape)\n",
    "            batch_pred = list(y_pred.argmax(axis = 1).cpu().detach().numpy())\n",
    "            batch_labels = list(l.cpu().detach().numpy())\n",
    "            #print(batch_pred.shape, batch_labels.shape)\n",
    "            preds = preds + batch_pred\n",
    "            labels = labels + batch_labels\n",
    "        \n",
    "    val_acc = accuracy_score(labels, preds)\n",
    "    print(f\"vall accuracy is {val_acc}\") \n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        patience = 3\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model, 'model.pth')\n",
    "        print(f\"Model saved at {i} epoch\")\n",
    "        \n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print(\"Early stopping...\")\n",
    "            break\n",
    "        \n",
    "    \n",
    "    scheduler.step(val_acc)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/kaggle/input/silero-audio-classifier/sample_submission.csv')\n",
    "test_data_path = '/kaggle/input/silero-audio-classifier/val'\n",
    "test_dataset = SoundDataset(sub, test=True, data_path = test_data_path)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030fa9cf013e46528150e9072464a14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=863.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wav_path</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>val/e/b7cf2c4.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>val/0/8f1489d.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>val/f/14b7304.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>val/2/3763132.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>val/0/51c4271.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            wav_path  target\n",
       "0  val/e/b7cf2c4.wav       2\n",
       "1  val/0/8f1489d.wav       2\n",
       "2  val/f/14b7304.wav       0\n",
       "3  val/2/3763132.wav       0\n",
       "4  val/0/51c4271.wav       1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'model.pth'\n",
    "model = torch.load(model_path)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "model.eval()\n",
    "preds = []\n",
    "\n",
    "tkloader = tqdm(test_loader, total = len(test_loader))\n",
    "for x in tkloader:\n",
    "    with torch.no_grad():\n",
    "        logits = model(x.to(device)).cpu()\n",
    "        preds.append(logits)\n",
    "        \n",
    "labels = torch.cat(preds, axis = 0).argmax(axis = 1)\n",
    "labels.detach().numpy()\n",
    "\n",
    "sub = pd.read_csv('/kaggle/input/silero-audio-classifier/sample_submission.csv')\n",
    "sub['target'] = labels\n",
    "\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "030fa9cf013e46528150e9072464a14b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_856fb67dc63848e595dad33038fc3fa8",
        "IPY_MODEL_31f907085f63439da82fb65beabbe641"
       ],
       "layout": "IPY_MODEL_6b37299101b34d878bd36f2e232fbe91"
      }
     },
     "1a58714f9bc345d0998f3ace788d4dd8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1cdf971ec4414c2fa094954d23a1d56a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2f497983a69f413881ae408b62555247": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "31f907085f63439da82fb65beabbe641": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fb0f337687cd4a2587ed58fd44f048b0",
       "placeholder": "​",
       "style": "IPY_MODEL_1cdf971ec4414c2fa094954d23a1d56a",
       "value": " 863/863 [08:12&lt;00:00,  1.75it/s]"
      }
     },
     "6b37299101b34d878bd36f2e232fbe91": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "856fb67dc63848e595dad33038fc3fa8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1a58714f9bc345d0998f3ace788d4dd8",
       "max": 863,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2f497983a69f413881ae408b62555247",
       "value": 863
      }
     },
     "fb0f337687cd4a2587ed58fd44f048b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
