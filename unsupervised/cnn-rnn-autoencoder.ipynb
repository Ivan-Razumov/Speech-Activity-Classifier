{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.io.wavfile import read\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PART = 0.05 # part of full data used to train our model \n",
    "VAL_PART = 0.2 # part of train data for validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/silero-audio-classifier/train.csv')\n",
    "_, train_df, _, _ = train_test_split(train_df, train_df['label'].values, test_size = TRAIN_PART, stratify = train_df['label'].values)\n",
    "train, val, _, _ = train_test_split(train_df, train_df['label'].values, test_size = VAL_PART, stratify = train_df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(label, num_class = 3):\n",
    "    one_hot = torch.zeros(num_class)\n",
    "    one_hot[label] = 1\n",
    "    return one_hot\n",
    "\n",
    "def read_audio(path):\n",
    "            sr, wav = read(path)\n",
    "            assert sr == 16000\n",
    "            assert len(wav) == 16000 * 3\n",
    "            assert len(wav.shape) == 1\n",
    "            return wav\n",
    "        \n",
    "def read_audio_norm(path):\n",
    "            wav = read_audio(path)\n",
    "            abs_max = np.abs(wav).max()\n",
    "            wav = wav.astype('float32')\n",
    "            if abs_max > 0:\n",
    "                wav *= 1 / abs_max\n",
    "            return wav\n",
    "\n",
    "window_size = 0.02\n",
    "window_stride = 0.01\n",
    "sample_rate = 16000\n",
    "\n",
    "n_fft = int(sample_rate * (window_size + 1e-8))\n",
    "win_length = n_fft\n",
    "hop_length = int(sample_rate * (window_stride + 1e-8))\n",
    "\n",
    "kwargs = {\n",
    "    'n_fft': n_fft,\n",
    "    'hop_length': hop_length,\n",
    "    'win_length': n_fft\n",
    "}\n",
    "\n",
    "def stft(wav):\n",
    "    D = librosa.stft(wav,\n",
    "                     **kwargs)\n",
    "    mag, phase = librosa.magphase(D)    \n",
    "    return mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, test = False, data_path = '/kaggle/input/silero-audio-classifier/train'):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "                \n",
    "        label_dict = {'speech':0,\n",
    "                      'music':1,\n",
    "                      'noise':2}\n",
    "        \n",
    "        wav = read_audio_norm(os.path.join(self.data_path, self.df.iloc[idx].wav_path))\n",
    "        wav = torch.tensor(wav).unsqueeze(0)\n",
    "        \n",
    "        return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_ch = 64, out_ch = 128):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1,n_ch,10,5),\n",
    "            nn.GroupNorm(1, n_ch),\n",
    "            nn.ReLU(), \n",
    "            \n",
    "            nn.Conv1d(n_ch,n_ch,8,4),\n",
    "            nn.GroupNorm(1, n_ch),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(n_ch,n_ch,4,2),\n",
    "            nn.GroupNorm(1, n_ch),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(n_ch,n_ch,4,2),\n",
    "            nn.GroupNorm(1, n_ch),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(n_ch, out_ch,4,2,padding = 2),\n",
    "            nn.GroupNorm(1, out_ch),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_ch = 64, out_ch = 128):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            \n",
    "            nn.ConvTranspose1d(out_ch,n_ch,4,2, padding = 1, output_padding = 0),\n",
    "            nn.GroupNorm(1, n_ch),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose1d(n_ch,n_ch,4,2, padding = 1, output_padding = 0),\n",
    "            nn.GroupNorm(1, n_ch),\n",
    "            nn.ReLU(), \n",
    "            \n",
    "            nn.ConvTranspose1d(n_ch,n_ch,4,2, padding = 1, output_padding = 0),\n",
    "            nn.GroupNorm(1, n_ch),\n",
    "            nn.ReLU(), \n",
    "            \n",
    "            nn.ConvTranspose1d(n_ch,n_ch,8,4, padding = 2, output_padding = 0),\n",
    "            nn.GroupNorm(1, n_ch),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose1d(n_ch,1,10,5, padding = 3, output_padding = 1),\n",
    "            nn.GroupNorm(1, 1),\n",
    "            nn.ReLU(), \n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNAutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size = 128, trg_len = 300, h_size = 128, n_layers = 2, bidirectional = True, device = 'cpu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_size = in_size\n",
    "        self.trg_len = trg_len\n",
    "        self.device = device\n",
    "        n_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        self.rnn_encoder = nn.GRU(input_size = in_size, hidden_size = h_size, batch_first = True, num_layers = n_layers, bidirectional = bidirectional)\n",
    "        self.rnn_decoder = nn.GRU(input_size = in_size, hidden_size = h_size, batch_first = True, num_layers = n_layers, bidirectional = bidirectional)\n",
    "        \n",
    "        self.rnn_decoder_output = nn.Linear(h_size*n_directions, in_size)\n",
    "        \n",
    "        self.cnn_encoder = CNNEncoder()\n",
    "        self.cnn_decoder = CNNDecoder()\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def encode(self, x):\n",
    "        z = self.cnn_encoder(x).permute(0, 2, 1)\n",
    "        _, h = self.rnn_encoder(z)\n",
    "        return h\n",
    "    \n",
    "    \n",
    "    def decode(self, h):\n",
    "        \n",
    "        batch_size = h.shape[1]\n",
    "        outputs = torch.zeros(batch_size, 1, self.in_size).to(self.device)\n",
    "        \n",
    "        for t in range(1, self.trg_len + 1):\n",
    "            output, h = self.rnn_decoder(outputs[:,-1,:].unsqueeze(1), h)\n",
    "            output = self.rnn_decoder_output(output[:,-1,:]).unsqueeze(1)\n",
    "            outputs = torch.cat([outputs, output], axis = 1)\n",
    "        \n",
    "        #print(outputs.shape)\n",
    "        outputs = self.cnn_decoder(outputs[:,1:].permute(0,2,1))\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden = self.encode(x)\n",
    "        outputs = self.decode(hidden)\n",
    "        return outputs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SoundDataset(train)\n",
    "val_dataset = SoundDataset(val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 15\n",
    "lr = 3e-4\n",
    "\n",
    "model = RNNAutoEncoder(device = device)\n",
    "optimizer = Adam(model.parameters(), lr = lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer=optimizer, mode='min', patience=1, verbose=True, factor=0.2)\n",
    "criterion = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0...\n",
      "Epoch 0 train loss is 0.05803881448350454\n",
      "\n",
      "Validation...\n",
      "Epoch 0 val loss is 0.04501417214267476\n",
      "\n",
      "Model saved at 0 epoch\n",
      "\n",
      "\n",
      "\n",
      "Training epoch 1...\n",
      "Epoch 1 train loss is 0.043013304292240685\n",
      "\n",
      "Validation...\n",
      "Epoch 1 val loss is 0.04448712435226108\n",
      "\n",
      "Model saved at 1 epoch\n",
      "\n",
      "\n",
      "\n",
      "Training epoch 2...\n",
      "Epoch 2 train loss is 0.04281011086000859\n",
      "\n",
      "Validation...\n",
      "Epoch 2 val loss is 0.044476193809058776\n",
      "\n",
      "Model saved at 2 epoch\n",
      "\n",
      "\n",
      "\n",
      "Training epoch 3...\n",
      "Epoch 3 train loss is 0.04281146187139185\n",
      "\n",
      "Validation...\n",
      "Epoch 3 val loss is 0.044343429595925084\n",
      "\n",
      "Model saved at 3 epoch\n",
      "\n",
      "\n",
      "\n",
      "Training epoch 4...\n",
      "Epoch 4 train loss is 0.04274219055033741\n",
      "\n",
      "Validation...\n",
      "Epoch 4 val loss is 0.04429002619500077\n",
      "\n",
      "Model saved at 4 epoch\n",
      "\n",
      "\n",
      "\n",
      "Training epoch 5...\n",
      "Epoch 5 train loss is 0.042757562644866826\n",
      "\n",
      "Validation...\n",
      "Epoch 5 val loss is 0.04432337391081938\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training epoch 6...\n",
      "Epoch 6 train loss is 0.042715867491144886\n",
      "\n",
      "Validation...\n",
      "Epoch 6 val loss is 0.04441318749775027\n",
      "\n",
      "Epoch     7: reducing learning rate of group 0 to 6.0000e-05.\n",
      "\n",
      "\n",
      "\n",
      "Training epoch 7...\n",
      "Epoch 7 train loss is 0.042791308426673995\n",
      "\n",
      "Validation...\n",
      "Epoch 7 val loss is 0.04422120048209678\n",
      "\n",
      "Model saved at 7 epoch\n",
      "\n",
      "\n",
      "\n",
      "Training epoch 8...\n",
      "Epoch 8 train loss is 0.04271152101413549\n",
      "\n",
      "Validation...\n",
      "Epoch 8 val loss is 0.044187403110743956\n",
      "\n",
      "Model saved at 8 epoch\n",
      "\n",
      "\n",
      "\n",
      "Training epoch 9...\n",
      "Epoch 9 train loss is 0.04281083313062003\n",
      "\n",
      "Validation...\n",
      "Epoch 9 val loss is 0.04417975974637409\n",
      "\n",
      "Model saved at 9 epoch\n",
      "\n",
      "\n",
      "\n",
      "Training epoch 10...\n",
      "Epoch 10 train loss is 0.042717305649268\n",
      "\n",
      "Validation...\n",
      "Epoch 10 val loss is 0.04417920493802359\n",
      "\n",
      "Model saved at 10 epoch\n",
      "\n",
      "\n",
      "\n",
      "Training epoch 11...\n",
      "Epoch 11 train loss is 0.042697817413953315\n",
      "\n",
      "Validation...\n",
      "Epoch 11 val loss is 0.044168932709929554\n",
      "\n",
      "Model saved at 11 epoch\n",
      "\n",
      "\n",
      "\n",
      "Training epoch 12...\n",
      "Epoch 12 train loss is 0.04274027434052431\n",
      "\n",
      "Validation...\n",
      "Epoch 12 val loss is 0.04416506665997034\n",
      "\n",
      "Model saved at 12 epoch\n",
      "\n",
      "\n",
      "\n",
      "Training epoch 13...\n",
      "Epoch 13 train loss is 0.04271367749428017\n",
      "\n",
      "Validation...\n",
      "Epoch 13 val loss is 0.044161706166558484\n",
      "\n",
      "Model saved at 13 epoch\n",
      "\n",
      "\n",
      "\n",
      "Training epoch 14...\n",
      "Epoch 14 train loss is 0.04277762417185899\n",
      "\n",
      "Validation...\n",
      "Epoch 14 val loss is 0.0441481925027315\n",
      "\n",
      "Model saved at 14 epoch\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#tkepoch = tqdm(range(n_epoch), total = n_epoch)\n",
    "model.to(device)\n",
    "best_val_loss = np.float('inf')\n",
    "patience = 3\n",
    "\n",
    "for i in range(n_epoch):\n",
    "     \n",
    "    \n",
    "    print(f\"Training epoch {i}...\")\n",
    "    epoch_train_loss = []\n",
    "    model.train()\n",
    "    #tkloader = tqdm(train_loader, total = len(train_loader))\n",
    "    for x in train_loader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        x_pred = model(x)\n",
    "        \n",
    "        loss = criterion(x_pred, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_train_loss.append(loss.item())\n",
    "    \n",
    "    print(f\"Epoch {i} train loss is {sum(epoch_train_loss)/len(epoch_train_loss)}\\n\")\n",
    "    \n",
    "    print(\"Validation...\")\n",
    "    model.eval()\n",
    "    epoch_val_loss = []\n",
    "    #tkval = tqdm(val_loader, total = len(val_loader))\n",
    "\n",
    "    for x in val_loader:\n",
    "        with torch.no_grad():\n",
    "            x = x.to(device)\n",
    "            x_pred= model(x)\n",
    "            loss = criterion(x_pred, x)\n",
    "            epoch_val_loss.append(loss.item())\n",
    "        \n",
    "    ep_val_loss = sum(epoch_val_loss)/len(epoch_val_loss)\n",
    "    print(f\"Epoch {i} val loss is {ep_val_loss}\\n\")\n",
    "    \n",
    "    if ep_val_loss < best_val_loss:\n",
    "        patience = 3\n",
    "        best_val_loss = ep_val_loss\n",
    "        torch.save(model, 'model.pth')\n",
    "        print(f\"Model saved at {i} epoch\")\n",
    "        \n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print(\"Early stopping...\")\n",
    "            break\n",
    "        \n",
    "    \n",
    "    scheduler.step(ep_val_loss)\n",
    "    print(\"\\n\\n\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
